rook-ceph-cluster:
#  configOverride: |
#    [global]
#    mon_allow_pool_delete = true
#    osd_pool_default_size = 1
#    osd_pool_default_min_size = 1
  ingress:
    dashboard:
      annotations:
        # kubernetes.io/ingress.class: nginx
        # external-dns.alpha.kubernetes.io/hostname: example.com
        # nginx.ingress.kubernetes.io/rewrite-target: /ceph-dashboard/$2
        cert-manager.io/cluster-issuer: "letsencrypt-prod"
      # If the dashboard has ssl: true the following will make sure the NGINX Ingress controller can expose the dashboard correctly
        nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
        nginx.ingress.kubernetes.io/server-snippet: |
          proxy_ssl_verify off;
      host:
        name: ceph.k3s.sh4ke.rocks
        # path: "/ceph-dashboard(/|$)(.*)"
      tls:
        - hosts:
            - ceph.k3s.sh4ke.rocks
          secretName: ceph-k3s-sh4ke-rocks-tls
      ingressClassName: nginx
  cephClusterSpec:
#    mon:
#      count: 1
#      volumeClaimTemplate:
#        spec:
#          storageClassName: csi-cinder-sc-delete-all-zones
#          resources:
#            requests:
#              storage: 10Gi
    crashCollector:
      disable: true
    storage:
      storageClassDeviceSets:
        - name: set1
          # The number of OSDs to create from this device set
          count: 3
          # IMPORTANT: If volumes specified by the storageClassName are not portable across nodes
          # this needs to be set to false. For example, if using the local storage provisioner
          # this should be false.
          portable: false
          # Certain storage class in the Cloud are slow
          # Rook can configure the OSD running on PVC to accommodate that by tuning some of the Ceph internal
          # Currently, "gp2" has been identified as such
          tuneDeviceClass: true
          # Certain storage class in the Cloud are fast
          # Rook can configure the OSD running on PVC to accommodate that by tuning some of the Ceph internal
          # Currently, "managed-premium" has been identified as such
          tuneFastDeviceClass: false
          # whether to encrypt the deviceSet or not
          encrypted: false
          # Since the OSDs could end up on any node, an effort needs to be made to spread the OSDs
          # across nodes as much as possible. Unfortunately the pod anti-affinity breaks down
          # as soon as you have more than one OSD per node. The topology spread constraints will
          # give us an even spread on K8s 1.18 or newer.
          placement:
            topologySpreadConstraints:
              - maxSkew: 1
                topologyKey: topology.cinder.csi.openstack.org/zone
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd
          preparePlacement:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: app
                          operator: In
                          values:
                            - rook-ceph-osd
                        - key: app
                          operator: In
                          values:
                            - rook-ceph-osd-prepare
                    topologyKey: topology.cinder.csi.openstack.org/zone
            topologySpreadConstraints:
              - maxSkew: 1
                # IMPORTANT: If you don't have zone labels, change this to another key such as kubernetes.io/hostname
                topologyKey: topology.cinder.csi.openstack.org/zone
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd-prepare
          resources:
          # These are the OSD daemon limits. For OSD prepare limits, see the separate section below for "prepareosd" resources
          #   limits:
          #     cpu: "500m"
          #     memory: "4Gi"
          #   requests:
          #     cpu: "500m"
          #     memory: "4Gi"
          volumeClaimTemplates:
            - metadata:
                name: data
                # if you are looking at giving your OSD a different CRUSH device class than the one detected by Ceph
                # annotations:
                #   crushDeviceClass: hybrid
              spec:
                resources:
                  requests:
                    storage: 100Gi
                # IMPORTANT: Change the storage class depending on your environment
                storageClassName: csi-cinder-sc-delete-all-zones
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
          # dedicated block device to store bluestore database (block.db)
          # - metadata:
          #     name: metadata
          #   spec:
          #     resources:
          #       requests:
          #         # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing
          #         storage: 5Gi
          #     # IMPORTANT: Change the storage class depending on your environment
          #     storageClassName: io1
          #     volumeMode: Block
          #     accessModes:
          #       - ReadWriteOnce
          # dedicated block device to store bluestore wal (block.wal)
          # - metadata:
          #     name: wal
          #   spec:
          #     resources:
          #       requests:
          #         # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing
          #         storage: 5Gi
          #     # IMPORTANT: Change the storage class depending on your environment
          #     storageClassName: io1
          #     volumeMode: Block
          #     accessModes:
          #       - ReadWriteOnce
          # Scheduler name for OSD pod placement
          # schedulerName: osd-scheduler
      # when onlyApplyOSDPlacement is false, will merge both placement.All() and storageClassDeviceSets.Placement.
      onlyApplyOSDPlacement: false
